\fancyhead[LE]{\textbf{\thepage}}
\fancyhead[RE]{\textit{Introduction}}
\fancyhead[LO]{\textit{Introduction}}
\fancyhead[RO]{\textbf{\thepage}}

Natural Language Processing (NLP) is a branch of Artificial Intelligence that
aims to make machines able to process and understand information from the human
language in order to solve some specific tasks, like translating a document or
automatically writing reports from market data. In the context of this thesis,
we focus on textual information, which can be found in webpages, books,
communication applications, etc. With the democratization of the Internet, the
amount of written and shared textual information has never been so important.
For example, each second\footnote{As of March 2020, according to
\url{https://www.internetlivestats.com/one-second/} .}, almost 9,000
tweets\footnote{A \textit{tweet} is a short message (up to 280 characters)
posted on the website \url{https://www.twitter.com} .} are published and almost
3 million emails are sent. Being able to process this large amount of data to
extract meaningful information can no longer be achieved manually, and for this
reason, the development of NLP models have rapidly gained interest within the
last couple of years to do it automatically. These models have become more and
more complex in order to solve more and more difficult tasks, like with
self-generating dialog systems (chatbots) or with personal assistants (Siri,
Alexa, etc.). Unlike humans, these models do not directly process words to solve
a given task. Most of the NLP models are based on Machine Learning (a
subfield of Artificial Intelligence) models which commonly use vectors as a
representation of data in order to apply some algorithms and solve the task.
Therefore, before solving a task, a common preliminary step of NLP models is to
represent the elements one can find in textual information as vector
representations. \medskip

In this thesis, we are interested in methods used to generate those vector
representations of textual information, and more specifically, representations
of words since they are the most basic unit one can find in a text. These word
representations are called \textit{word embeddings} and are typically
represented as an array of values. Word embeddings are then used in downstream
NLP models to solve tasks. The values in word embeddings have to be set such
that they reflect the linguistic properties of words and the relations between
them. Indeed, they are the main source of information the downstream models have
access to in order to solve the task. Without linguistic information encoded
into word embeddings, the models would not have any knowledge to know how to use
the word vectors and give the expected answer or the correct prediction for a
given task. The first works on word embeddings manually created word vectors by
using specific linguistic properties of words, generated by linguistic experts.
However, these methods are not scalable when the number of words to embed is in
the order of millions, which is commonly the case for popular languages like
English and the massive amount of textual data available on the Web. Therefore,
to overcome this shortcoming, some methods have been developed to automatically
learn word embeddings. They are usually based on the statistics of the
occurrences of words in texts to learn word representations that convey
linguistic information. These methods are mostly \textit{unsupervised}: there
are no clues or hints given to the method to know which linguistic information
should be encoded into the embeddings, or which word vectors should be related.
\bigbreak

During the last decade, many different methods have been created to learn word
embeddings that capture linguistic information in order to be used in downstream
NLP models to solve tasks. Among these methods, two main limitations can be
observed:

\begin{itemize}
  \item Most of the methods learn word embeddings with statistics from large
    training texts extracted from the Web. These texts are, for the majority,
    generic and do not contain many specific linguistic information, which is
    therefore not captured into the word embeddings learned by these methods.
  \item As the tasks to solve become more difficult, the linguistic knowledge
    required to solve them increases, and so is the complexity and the size of
    the methods that learn to encode this additional knowledge into word
    embeddings. With the democratization of NLP applications running on
    low-resource devices like smartphones, such complex and large word
    representations models cannot be used on those devices.
\end{itemize}

\noindent These two limitations of current word embeddings are complementary: if
more linguistic information is encoded into word vectors with other sources of
information, the representations would become larger and would not be able to
run on low-resource devices. On the other hand, if the word vectors are reduced
by removing some of their values to be able to run on low-resource devices, the
amount of information they encode would also be reduced. The contributions
presented in this thesis address each one of these limitations.

\subsubsection{Contributions}
  This thesis presents two contributions to learn word embeddings which address
  the two aforementionned limitations of current word embeddings. The first
  contribution is a novel method that uses lexical dictionaries to extract
  semantic relations between words and incorporate this additional information
  in a model to learn word embeddings. The process of extracting information
  from dictionaries is based on the cooccurrence of words in their dictionary
  definitions and is completely automatic. The word embeddings learned with this
  method exhibit significant improvements in word semantic similarity tasks
  compared to other common word embeddings learning methods. This first
  contribution has been accepted and presented at the EMNLP 2017 conference
  \citep{tissier2017dict2vec}. The second contribution is a method to transform
  common word embeddings (which usually use real values to encode information)
  into binary word vectors. The binary vectors are much smaller in memory than
  the real-valued ones (more than 30 times smaller) and have the advantage of
  accelerating vector operations, which are two of the main characteristics
  required to be used in downstream models on low-resource devices. This second
  contribution has been accepted and presented at the AAAI 2019 conference
  \citep{tissier2019near}.

\subsubsection{Outline}
  This thesis is divided into three main parts:~\autoref{chap:preliminaries}
  and~\autoref{chap:ml-for-we} introduce the main notions and concepts used
  throughout this thesis;~\autoref{chap:methods-we}
  and~\autoref{chap:methods-reduction} give an overview of the existing methods
  to learn word embeddings and reduce the size of
  representations;~\autoref{chap:dict2vec} and~\autoref{chap:nlb} present the
  two contributions of this thesis. \medskip

    \textbf{Chapter 1} introduces the main notions used in this thesis, the most
    important one being the definition of \textit{word embeddings}. It also
    details tasks to evaluate the quality of word embeddings, which are used in
    the chapters of contributions (\autoref{chap:dict2vec}
    and~\autoref{chap:nlb}). \medskip

    \textbf{Chapter 2} presents the main concepts of machine learning:
    supervised learning, unsupervised learning and semi-supervised learning. It
    also explains how can machine learning models learn from data and presents
    two models (neural network and autoencoder) that are commonly used in the
    word embeddings literature. \medskip

    \textbf{Chapter 3} is an overview of the most common existing methods to
    learn word embeddings. It details different models, from the first works on
    word embeddings to the latest ones which are, as we said before, complex and
    large. Some of the methods presented in this chapter are used
    in~\autoref{chap:dict2vec} to compare the performances of the word
    embeddings learned by the model of the first contribution. \medskip

    \textbf{Chapter 4} is an overview of existing methods to reduce the size of
    vector representations so they can be smaller in memory or speed up vector
    computations. Some of the methods presented in this chapter are used
    in~\autoref{chap:nlb} to compare the performances of the binary word vectors
    learned by the model of the second contribution. \medskip

    \textbf{Chapter 5} presents the first contribution of this thesis. It
    details how additional semantic information is extracted from the dictionary
    definitions of words and how this information is used to learn
    semantically-richer word embeddings. Several tasks like word semantic
    similarity or document classification are used to evaluate the quality of
    those word representations and their performances in downstream models.
    \medskip

    \textbf{Chapter 6} presents the second contribution of this thesis. It
    details the model and its architecture used to transform learned word
    embeddings into binary word vectors of any size. Several evaluations are
    performed to measure the quality of those binary vectors and an additional
    task is performed to evaluate the computational benefits of binary vectors
    for semantic similarity computations.
